---
title: "On the exactness of permutation tests"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{On the exactness of permutation tests}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, message=FALSE}
library(magrittr)
library(purrr)
library(psi)
```

In this article, we discuss the exactness property of permutation tests, which is closely related to how $p$-values are estimated from the permutations.

## Exactness of a test

When you want to perform an hypothesis test, you first need to specify the **null hypothesis** $H_0$ you are testing and the **alternative hypothesis** $H_a$ you are testing against. Next, you need to define a **statistic** $T$ that fulfills the following properties:

- you can compute its observed value under the null hypothesis once you observed some data;
- large values of the statistic are evidence in favor of the alternative hypothesis;
- you know (an approximation of) the distribution of $T$ under the null hypothesis, also known as the **null distribution**.

Finally, you specify a significance level $\alpha$ which controls the probability of making type I errors:
$$  \mathbb{P} \left( \mbox{Type I errors} \right) = \mathbb{P} \left( \mbox{Reject } H_0 \mbox{ while it was true} \right) = \alpha. $$
This control is exact (i.e. the equality holds) if you know the null distribution. When you estimate it, as it is done in resampling-based testing procedures, you are not guaranteed to get this control.

Permutation tests use permutations of the data to estimate the null distribution and therefore access an estimate of the $p$-value. There are several ways of estimating $p$-values from a permutation scheme. In this article, we first review the traditional approach to permutation testing and show that it does not lead to an exact testing procedure. Next, we demonstrate that we can make the permutation test exact by estimating the $p$-value in a different fashion.

## Monte-Carlo estimator of the p-value

Let us state again the two-sample testing problem for the sake of clarity.

We start with two samples $X_1, \dots, X_{n_x} \stackrel{iid}{\sim} \mathcal{D}(\theta_x)$ and $Y_1, \dots, Y_{n_y} \stackrel{iid}{\sim} \mathcal{D}(\theta_y)$. We want to know whether the two distributions are the same or not on the basis of the two samples we collected. In this parametric setting, it boils down to testing the following hypotheses:
$$ H_0: \theta_x = \theta_y \quad \mbox{vs} \quad \theta_x \neq \theta_y. $$
Let $T$ be a statistic that depends on the two samples which is suited for elucidating this test, i.e.:

- you can compute its observed value under the null hypothesis once you observed some data;
- large values of the statistic are evidence in favor of the alternative hypothesis.

Now, for performing the test, one should also know (an approximation of) the distribution of $T$ under the null hypothesis, also known as the **null distribution**, from which the p-value associated to this test can be computed for instance.

If one knows the exact null distribution, then there is no need to resort to permutations. However, if the null distribution is not known, permutations come in handy for approaching it. 

The idea is that, under the null hypothesis, the two samples come from the same distribution. Hence, we can pull them together as one big sample of size $n = n_x + n_y$ generated by that common distribution. At this point, we can split the pooled sample into two random subsets of size $n_x$ and $n_y$ respectively, and use them to compute a value of the statistic $T$. If we repeat many times this splitting strategy, say $B$ times, we end up with $B$ values of the statistic from which we can compute the empirical distribution, known as the **permutation distribution**, which approaches the null distribution.

Following this description, if we denote $t_\mathrm{obs}$ the value of the statistic computed from the original two samples and $t_b^\star$ the value of the statistic computed from the two samples generated at the $b$-*th* split of the pooled sample, we deduce that an estimate of the p-value can be obtained by:
$$ \widehat{p}_\mathrm{MC} = \frac{1}{B} \sum_{b=1}^B \mathbb{I}_{[t_\mathrm{obs},+\infty)}(t_b^\star). $$

However, when one uses this estimator of the p-value for the purpose of hypothesis testing, the resulting test is not exact. Let us provide some empirical evidence by running simulations. In **psi**, you perform a permutation test using this estimator of the p-value by setting the optional argument `test` to `"approximate"`.

```{r power-simulation}
alpha <- 0.05
B <- 100
set.seed(1234)
1:B %>% 
  map_lgl(~ {
    x <- rnorm(n = 10, mean = 0, sd = 1)
    y <- rnorm(n = 10, mean = 0, sd = 1)
    test_hotelling <- two_sample_test(
      x = x, 
      y = y, 
      statistic = stat_hotelling, 
      test = "exact"
    )
    test_hotelling$pvalue < alpha
  }) %>% 
  mean()
set.seed(1234)
1:B %>% 
  map_lgl(~ {
    x <- rnorm(n = 10, mean = 0, sd = 1)
    y <- rnorm(n = 10, mean = 0, sd = 1)
    test_hotelling <- two_sample_test(
      x = x, 
      y = y, 
      statistic = stat_hotelling, 
      test = "approximate"
    )
    test_hotelling$pvalue < alpha
  }) %>% 
  mean()
```
